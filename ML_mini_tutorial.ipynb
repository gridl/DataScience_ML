{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-step mini tutorial on Machine learning using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import the libraries we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pandas for data manipulation\n",
    "import pandas\n",
    "\n",
    "# plotting purposes\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# machine learning\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Loading the data set\n",
    "In this case we will be working with the iris data set. Since we are using seaborn and we have already lodaded the library we will load the data set directly from  `sns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_data = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarising and pre-processing the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by exploring the data, some of the things we will be doing include:\n",
    "1. Have a look at the data contained in the data set\n",
    "2. Find out the dimensions of the data set\n",
    "3. Perform a basic statistical summary of the data\n",
    "4. Break the data down by the `species` variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data set dimension\n",
    "We can have a very quick look at number of intances and attributes (rows and columns respectively) conatined in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting the shape \n",
    "print(my_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Exploring the contained data\n",
    "We start by looking at the kind of data types included in the set as well as 'eyeballing' the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting info on the data contained\n",
    "my_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# seeing the first 20 rows of the data\n",
    "# note the dafult no. of rows shown is 10, here \n",
    "# we overwrote this by specifying the no. of columns\n",
    "my_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Statistical summary\n",
    "The next step will be to perform a basic statistical summary of the data attributes. \n",
    "\n",
    "This will contain the counts, mean, minimum, maximum and the main percentiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case all the numerical values have the same scale (cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Class distribution\n",
    "Now we will have a look at the number of rows that belong to each species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_data.groupby('species').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each species contains 50 instances "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a basic idea of the data contained in the dataset we will create some plots that will help us to understant the information contained in the set.\n",
    "\n",
    "There are 2 kinds of plots we will be working with:\n",
    "1. Univariate plots: one attribute at a time\n",
    "2. Multivariate plots: unveil the relationship between attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Univariate plots\n",
    "\n",
    "First I am going to specify the color palette I will be using. If you know which palette you want you can choose direcly, otherwise you can call the widget as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "palette = sns.choose_colorbrewer_palette('qualitative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to specify that we want the plots to be displayed in line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define some seaborn preferences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_palette(palette)\n",
    "sns.set_context('notebook', font_scale = 1.5)\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Box plot**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = sns.boxplot(data =  my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Histogram**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = my_data.hist(edgecolor = 'black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Multivariate plots\n",
    "\n",
    "Now we will starta having a look at the relationship between the variables.\n",
    "\n",
    "First let have a look at the scatterplots of all pair of attributes. This will allow us to see the relationship between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " with sns.axes_style('ticks'):\n",
    "    p = sns.pairplot(my_data, hue = 'species', diag_kind = 'kde')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal groups pairs of attributes, note we can use `hist` or `kde` types for the diagonal plots. We can further visualize ths correlation using scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.PairGrid(my_data, hue = 'species')\n",
    "g.map(plt.scatter, edgecolor = 'black', alpha = 0.7)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by deafult the seaborn `pairGrid` function uses Histograms on the diagonal relationships while `facetgrid` is not limited to histograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating some algorithms\n",
    "\n",
    "We will use some algorithms here and evaluate their accuracy on the data set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create a validation data set\n",
    "We need to know that the model we created is any good.\n",
    "\n",
    "Later, we will use statistical methods to estimate the accuracy of the models that we create on unseen data. We also want a more concrete estimate of the accuracy of the best model on unseen data by evaluating it on actual unseen data.\n",
    "\n",
    "That is, we are going to hold back some data that the algorithms will not get to see and we will use this data to get a second and independent idea of how accurate the best model might actually be.\n",
    "\n",
    "We will split the loaded dataset into two, 80% of which we will use to train our models and 20% that we will hold back as a validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "\n",
    "array = my_data.values\n",
    "\n",
    "# data vaues\n",
    "X = array[:,0:4]\n",
    "\n",
    "# corresponds to the species (the target of the data set)\n",
    "Y = array[:,4]\n",
    "\n",
    "validation_size = 0.20\n",
    "seed = 5\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a training data contained in `X_train` and `Y_train` as well as validation sets (`Y_validation`, `X_validation`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Test harness\n",
    "\n",
    "In this case I have chosen to use a 10-fold cross validation to estimate the accuracy of the models. \n",
    "\n",
    "This means that we will split the dataset in 10 different subsets: 9 to train and 1 for testing and repeat for all combination of train/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "\n",
    "# this will be used later to evaluate each algorithm\n",
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the metric of ‘accuracy‘ to evaluate models. \n",
    "\n",
    "This is a ratio of the number of correctly predicted instances divided by the total number of instances in the dataset multiplied by 100 to give a percentage (e.g. 95% accurate). \n",
    "\n",
    "$$accuracy = \\frac{correct_{pred}}{total_{inst}} x 100$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4.3 Building the models\n",
    "\n",
    "\n",
    "6 different algorithms are considered:\n",
    "1. Logistic regression (LR)\n",
    "2. Linear Discriminant Analysis (LDA)\n",
    "3. K-nearest Neighbours (KNN)\n",
    "4. Classification trees (CT)\n",
    "5. Gaussian Naive Bayes (NB)\n",
    "6. Support Vector Machines (SVM)\n",
    "\n",
    "Such algorithms provide a nice mixture of linear and non-linear algorithms. \n",
    "\n",
    "We will use a random seed before each run to ensure that theevaluation of each algorithm is performed using exactly the same data splits. It ensures the results are directly comparable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building the algorithms\n",
    "alg = []\n",
    "\n",
    "alg.append(('LR', LogisticRegression()))\n",
    "alg.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "alg.append(('KNN', KNeighborsClassifier()))\n",
    "alg.append(('CT', DecisionTreeClassifier()))\n",
    "alg.append(('NB', GaussianNB()))\n",
    "alg.append(('SVM', SVC()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate each algorithm\n",
    "results = []\n",
    "alias_alg = []\n",
    "\n",
    "for name, algorithm in alg:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(algorithm, X_train, Y_train, cv = kfold, scoring = scoring)\n",
    "    results.append(cv_results)\n",
    "    alias_alg.append(name)\n",
    "    \n",
    "    status = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From the initial results we can see that 'KNN' and 'LDA' have the highest estimation accuracy. \n",
    "\n",
    "We are now going to create a plot of the algorithms results and compare their mean accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = sns.boxplot(data = results).set_xticklabels(alias_alg)\n",
    "fig.suptitle('ML Algorithm comparison')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Making  predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KK algorithm seems to be the most accurate model when used on the training data sets. \n",
    "\n",
    "Now we will apply the models on the validation sets. This will give us an indication of the algorithm accuracy. \n",
    "\n",
    "It is often advisable to keep a validation data set in case you over fit the training data set. \n",
    "\n",
    "We can run the KNN model directly on the validation set and summarize the results as a final accuracy score, a confusion matrix and a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "predictions = knn.predict(X_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(accuracy_score(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is around 0.9 (90%). Now we will have a look  at the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(Y_validation, predictions)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix provides information on:\n",
    "1. The number of correct predictions for each class (in this case the species)\n",
    "2. The number of incorrect predictions for each class (arranged by the class they predicted)\n",
    "\n",
    "These counts are organized as seen above in a matrix as follows:\n",
    "* **Expected down the side**: each row of the matrix corresponds to an actual class\n",
    "* **Predicted across the top**: each column represents a predicted class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be worth then plotting the confusion matrix to better understand this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# changing the colormap\n",
    "my_cmap = sns.light_palette((210, 90, 60), input=\"husl\", as_cmap=True)\n",
    "\n",
    "\n",
    "ax = sns.heatmap(cm, annot= True, fmt = '.2f', xticklabels= my_data.species.unique(),\n",
    "                yticklabels= my_data.species.unique(), cmap = my_cmap)\n",
    "\n",
    "# setting axes and plot titles\n",
    "ax.set_xlabel('Predicted class')\n",
    "ax.set_ylabel('Real class')\n",
    "ax.set_title('Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wnated to normalize the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm_n = cm / cm.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# plotting the normalized confusion matrix\n",
    "ax = sns.heatmap(cm_n, annot= True, fmt = '.2f', xticklabels= my_data.species.unique(),\n",
    "                yticklabels= my_data.species.unique(), cmap = my_cmap)\n",
    "\n",
    "# setting axes and plot titles\n",
    "ax.set_xlabel('Predicted class')\n",
    "ax.set_ylabel('Real class')\n",
    "ax.set_title('Confusion matrix normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "347a28b44e394192b360fc19de816fd5": {
     "views": [
      {
       "cell_index": 21
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
